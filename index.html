<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Enhancing Robotic Skill Acquisition</title>
    
    <!-- 加载最新 jQuery -->
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>

    <style>
        body {
            font-family: Arial, sans-serif;
            text-align: center;
            max-width: 1200px;
            margin: auto;
        }
        h1 {
            font-size: 32px;
            font-weight: bold;
        }
        .video-container {
            display: flex;
            justify-content: center;
            align-items: center;
            gap: 20px; /* GIF 之间的间距 */
            flex-wrap: wrap; /* 小屏幕自动换行 */
            margin-top: 20px;
        }
        img {
            max-width: 48%;
            height: auto;
            border-radius: 8px;
        }
        .paper-shadow {
            box-shadow: 0px 0px 5px rgba(0,0,0,0.3);
            padding: 10px;
            border-radius: 8px;
        }
        .center-table {
            margin: auto;
            width: 80%;
        }
    </style>
</head>
<body>

    <h1>Learning Multimodal Adaptive Grasping from Human Demonstrations with Visual, Tactile, and Gesture Guidance</h1>

    <!-- 研究人员信息 -->

    <!-- 代码 & 论文 -->

    <hr>

    <!-- Abstract -->
    <h1>Abstract</h1>
    <p>
        In recent years, the demand for dexterity and adaptability in robotic grasping and manipulation tasks has increased significantly. However, existing methods face challenges in multimodal perception and high degree-of-freedom manipulation control. This study presents a multimodal learning framework that enables a five-fingered dexterous hand to achieve adaptive grasping and human-level manipulation capabilities using human demonstration data. The proposed framework extracts spatiotemporal features from visual, haptic, and end-effector gesture modalities and integrates them with a large-scale vision-language model for task understanding and subtask planning. Human demonstration data are collected using a WiseGlove tactile glove and a RealSense camera, with the hand operation process segmented into five phases: reach, contact, manipulation, release, and placement. An adaptive algorithm is developed to dynamically adjust the fingertip force and grasping speed to address force control during manipulation based on object properties and real-time feedback. Experimental results demonstrate that the method achieves smooth transitions and stable manipulation across objects with varying stiffness and compliance. In addition, the approach supports cross-object and cross-task generalization, significantly enhancing the robustness and accuracy of dexterous grasping and manipulation. 
    </p>

    <hr>

</body>
</html>
